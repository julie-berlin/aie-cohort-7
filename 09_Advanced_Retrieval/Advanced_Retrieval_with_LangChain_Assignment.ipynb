{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- 🤝 Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- 🤝 Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# 🤝 Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/complaints.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Date received\",\n",
        "      \"Product\",\n",
        "      \"Sub-product\",\n",
        "      \"Issue\",\n",
        "      \"Sub-issue\",\n",
        "      \"Consumer complaint narrative\",\n",
        "      \"Company public response\",\n",
        "      \"Company\",\n",
        "      \"State\",\n",
        "      \"ZIP code\",\n",
        "      \"Tags\",\n",
        "      \"Consumer consent provided?\",\n",
        "      \"Submitted via\",\n",
        "      \"Date sent to company\",\n",
        "      \"Company response to consumer\",\n",
        "      \"Timely response?\",\n",
        "      \"Consumer disputed?\",\n",
        "      \"Complaint ID\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "loan_complaint_data = loader.load()\n",
        "\n",
        "for doc in loan_complaint_data:\n",
        "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loan_complaint_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    loan_complaint_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"LoanComplaints\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, particularly student loans as reflected in the complaints, appears to involve mismanagement and errors by lenders or servicers. This includes issues such as:\\n\\n- Errors in loan balances and balances growing despite payments\\n- Receiving bad or incorrect information about the loan\\n- Trouble with loan transfers and lack of proper notification\\n- Difficulty applying extra payments toward principal\\n- Problems with repayment plans or forbearance mismanagement\\n- Issues related to loan forgiveness or discharge eligibility\\n\\nOverall, the prevalent problem is mismanagement and inaccurate handling of loan information by servicers, leading to financial hardship, credit report inaccuracies, and lack of transparency.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, yes, some complaints did not get handled in a timely manner. Specifically, at least one complaint (Complaint ID: 12709087 involving MOHELA) was marked as \"Timely response?\": No, indicating it was not handled within the expected timeframe. Additionally, several other complaints mention delays or unresolved issues over extended periods, such as complaints about unresolved account corrections, unreturned calls, or unresolved disputes taking over a year.\\n\\nTherefore, the answer is: Yes, some complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans for several reasons, including:\\n\\n1. Accumulation of interest during forbearance or deferment periods, which increased the total amount owed and made repayment more difficult.\\n2. Lack of clear communication or proper notification from lenders or servicers about repayment start dates, loan transfers, or delinquency status, leading to unawareness of when payments were due.\\n3. Inability to afford increased or minimum payments due to stagnant wages, high living expenses, or financial hardship.\\n4. Mismanagement of loans by servicers, such as incorrect or confusing account information, failure to provide accurate payment or balance details, or improper handling of loan transfers.\\n5. Restrictions on applying extra payments directly to principal, which extended the repayment period and increased the total interest paid.\\n6. Errors or delays in processing income-based repayment plans or forbearance requests, leading to missed payments or credit issues.\\n7. Lack of eligibility or awareness of loan forgiveness programs, leaving borrowers burdened with unmanageable debt despite years of payments.\\n8. Psychological or informational barriers, including feeling misled about loan obligations, or not being adequately informed about interest accrual and repayment options.\\n\\nOverall, a combination of financial hardships, inadequate communication, and administrative difficulties contributed to many borrowers' inability to meet their loan repayment obligations.\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be dealing with the lender or servicer, specifically related to disputes over fees, interest calculations, payment application, and inaccurate or bad information about the loan. Many complaints involve difficulties in understanding or managing repayment terms, being misled about loan details, or issues with payment processing.\\n\\nIf I had to identify a single most common issue from the data, it would be: **Problems related to dealing with the lender or servicer, including disputes over fees, interest, and loan information.**\\n\\nPlease note that this is based on the recurring theme observed in multiple complaints within the context.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, all the complaints listed indicate that the companies responded to the complaints in a timely manner, with responses marked as \"Yes\" under the \"Timely response?\" field. Therefore, there is no indication that any complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans primarily because of issues related to miscommunication, mismanagement, and administrative problems with loan servicers. For example, some borrowers' automatic payments were unenrolled without their knowledge, leading to missed payments and negative impacts on their credit scores. Others were steered into incorrect forbearance options or experienced delays and lack of response when attempting to apply for relief, deferments, or forbearance. Additionally, some borrowers were not properly informed about changes in their loan servicing, new account details, or the status of their payments, which contributed to unpaid balances and deteriorating credit. Overall, these issues stem from ineffective communication, mishandling by servicers, and procedural errors within the loan servicing system.\""
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer.\n",
        "\n",
        "##### ✅ **Answer:**\n",
        "\n",
        "Given the question *\"What does error code TS-999 mean?\"* for a corpus including error documentation and many specific types of error codes:\n",
        "\n",
        "- An embedding model will find chunks related to error codes in general but may not find the exact match to \"TS-999\" which is very important in this query.\n",
        "\n",
        "- BM25 will return chunks that contain the exact phrase \"TS-999\"\n",
        "\n",
        "These differences are fundamental to these two types of preprocessing. Embedding models are designed to extract semantic meaning. BM25 is short for \"Best Matching 25\" which is a ranking function that uses lexical matching to find precise word or phrase matches. It's particularly effective for queries that include unique identifiers or technical terms. \n",
        "\n",
        "If your use case is likely to require keyword exact matches, the advice is to use a hybrid approach and include both types of preprocessing (embedding and BM25 ranking) combined with rank fusion to get the best results from RAG.\n",
        "\n",
        "[Source: Anthropic Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to involve problems with loan servicing, including errors in loan balances, misapplied payments, incorrect or confusing information about loan terms, and mishandling of personal data. Many complaints highlight issues such as inaccurate account information, unauthorized transfers of loans, lack of communication or documentation, and violations of privacy laws.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, it appears that at least one complaint was not handled in a timely manner. Specifically, the complaint regarding the student loan account review and violations of FERPA has been open for nearly 18 months without resolution, despite requests for assistance. Although the company\\'s response indicated it was \"Closed with explanation\" and the response was \"Yes\" in terms of timeliness, the resolution has not been achieved for an extended period, suggesting that this complaint was not addressed promptly.\\n\\nAdditionally, there is a complaint from another individual about unresolved issues with their loan payments not being applied, which also seems to remain unresolved despite the complaint being submitted.\\n\\nTherefore, yes, some complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to a lack of clear communication, misinformation, and difficulties managing loan repayment options. Many borrowers were unaware that they needed to repay their loans because they were not properly informed by financial aid officers or loan servicers. Additionally, issues such as incorrect or inconsistent account information, unexplained increases in loan balances and interest, and limited repayment options like forbearance or deferment—while interest continued to accrue—made it challenging for borrowers to pay off their loans. Some borrowers also faced hardships because the available options often led to increasing total debt over time, and they lacked sufficient financial resources or information about how interest compounds and affects repayment, which further hindered their ability to successfully repay their loans.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided context, appears to be problems related to **debt management and reporting errors**, including:\\n\\n- Incorrect or misleading information on credit reports (e.g., account status incorrectly reported as delinquent or overdue).\\n- Systemic errors in credit reporting impacting credit scores.\\n- Confusion or disputes over loan balances, interest calculations, and loan transfer or servicing changes.\\n- Difficulties in communication with loan servicers, such as lack of transparency, unnotified loan transfers, and inadequate or misleading information about repayment options and loan terms.\\n- Challenges in handling forbearance, repayment plans, and unawarded or misapplied payments leading to increased balances and interest.\\n\\nThese issues often lead to significant financial hardship, damaged credit scores, and frustration with the loan servicing process, indicating that a most common problem is systemic mismanagement and errors in loan reporting and servicing.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the information provided, several complaints indicate that issues were not handled in a timely manner. For example, a complaint from a consumer involves a response time exceeding 1 year, with the complaint still unresolved despite requests for review and resolution. Additionally, some complaints note delays of over 30 days or more before receiving any response or resolution, despite the company\\'s responses being marked as \"timely\" or \"with explanation.\"\\n\\nTherefore, yes, there were complaints that did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans due to a variety of systemic and individual challenges, including:\\n\\n1. Lack of clear information and understanding about loan terms, interest accumulation, and repayment options, which led borrowers to make uninformed decisions.\\n2. High and accumulating interest, especially when loans were placed in forbearance or deferment, causing balances to grow and making repayment seem impossible.\\n3. Inadequate communication from lenders or servicers, resulting in borrowers being unaware of when repayment resumed or of available options such as income-driven repayment plans or loan forgiveness.\\n4. Misinformation or mismanagement by loan servicers, including incorrect account reporting, mishandling of payments, and improper transfer of loans, which further complicated repayment efforts.\\n5. Financial hardships, such as unemployment, illness, homelessness, or unexpected events, that made it difficult for borrowers to meet their repayment obligations.\\n6. Predatory practices like forbearance steering and coercive consolidation that increased loan balances instead of offering manageable repayment solutions.\\n7. Confusing or incomplete documentation and lack of access to loan information, hindering borrowers' ability to track balances or dispute errors.\\nOverall, these factors contributed to borrowers being unable to pay back their loans, not necessarily due to irresponsibility but often because of systemic flaws, miscommunication, and economic hardships.\""
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall.\n",
        "\n",
        "##### ✅ **Answer:**\n",
        "\n",
        "Multi-query retrieval improves recall by addressing the fundamental limitation that a single query formulation may not capture all relevant documents, even when those documents contain the information the user seeks. This is called the **vocabulary mismatch problem**. Users and document authors often use different terminology to describe the same concepts. Multi-query generates several reformulations of the original query, typically 3-5 variations that:\n",
        "\n",
        "- Use different synonyms and related terms\n",
        "- Vary the specificity level (broader or narrower focus)\n",
        "- Rephrase the question structure\n",
        "- Emphasize different aspects of the information\n",
        "\n",
        "This increased diversity in queries, in turn increases the chances of retrieving relevant documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = loan_complaint_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to be related to mismanagement and errors by loan servicers, including errors in loan balances, misapplied payments, wrongful denials of payment plans, and incorrect or inconsistent credit reporting. Many complaints highlight problems such as inaccurate loan balances, unfair increases in interest rates, and issues arising from the transfer or sale of loan accounts.'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, yes, there are complaints that did not get handled in a timely manner. Specifically, the complaints about the student loan issues with MOHELA (Complaint IDs: 12709087 and 12935889) indicate that the responses from the company were delayed, with the consumer noting they had not heard from anyone despite multiple follow-ups and extended wait times. The complaint about the credit dispute with Nelnet (Complaint ID: 13205525) was responded to within the expected timeframe, so that was handled timely.\\n\\nSo, the answer is: Yes, some complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans primarily due to financial hardships, lack of proper information, and mismanagement by loan servicers or educational institutions. Specifically, some borrowers experienced severe financial difficulties after graduation, making it impossible to consistently make payments. Others were misled by schools' representations about the value and manageability of their loans, and some faced issues with loan servicers failing to provide proper notification or account management, leading to unverified or questionable debts, late payments, or credit reporting errors. Additionally, some borrowers relied on deferment and forbearance options that increased interest costs, further complicating repayment.\""
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issues with student loans appear to involve:\\n\\n- Dealing with lenders or servicers, including receiving bad information about loans, incorrect account status, or mishandling of accounts.\\n- Problems related to loan management such as improper transfer, unclear or unnotified transfer of loan management, or incorrect reporting of loan status.\\n- Errors in loan balances or interest calculations, including unauthorized interest accrual, improper capitalization, or incorrect reporting to credit bureaus.\\n- Difficulties with repayment plans, including being steered into long-term forbearance without proper guidance on options like income-driven repayment or rehabilitation.\\n- Communications failures, such as lack of timely notification about default, transfer, or changes in account status.\\n- Issues with loan consolidation, including insufficient disclosure, lack of informed consent, or unexpected payment amounts.\\n\\nWhile there is no single \"most common\" issue explicitly highlighted, the recurring themes involve mismanagement and miscommunication by loan servicers, leading to errors in account information, improper reporting, or unfavorable loan handling practices.\\n\\nTherefore, the most common issue with loans, as reflected in these complaints, appears to be **mismanagement by loan servicers, including errors, miscommunication, and improper handling of loan information and account status**.\\n\\nIf you need a specific answer, it is this pattern of servicer misconduct, errors, and communication failures that is most prevalent.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the information provided, yes, there are complaints indicating that some complaints did not get handled in a timely manner. Specifically, at least one complaint (Complaint ID: 12935889) was marked \"No\" for timely response, meaning it was not handled promptly. Additionally, multiple complaints mention delays, lack of response, or insufficient follow-up from the companies involved.'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans primarily because of a combination of factors highlighted in the complaints:\\n\\n1. Lack of clear and adequate communication: Borrowers were often not informed about when payments were due, changes in loan servicers, or the status of their loans, leading to unintentional delinquency.\\n2. Mismanagement and errors by loan servicers: Many complaints describe errors such as misapplied payments, incorrect loan balances, and reporting delinquency to credit bureaus without proper notice or verification.\\n3. Unsuitable repayment options and strategies: Borrowers were frequently steered into forbearance or deferment without fully understanding the long-term consequences, including accumulating interest, which increased the total amount owed and extended repayment periods.\\n4. Difficulty in accessing accurate information: Complaints include issues like inability to obtain original loan documentation, proof of ownership, or correct account status, which impairs borrowers' understanding and management of their loans.\\n5. Economic hardships and unrealistic loan terms: Borrowers faced financial hardships, stagnant wages, and unmanageable payments, especially when interest accumulated due to forbearance or delayed repayment plans, making it impossible to pay off the loans.\\n6. Administrative and procedural failures: Reports of errors during loan transfers, improper handling of accounts, and failure to provide timely notices or proper documentation contributed to borrowers' inability to stay current.\\n7. Hidden or misunderstood loan conditions: Many borrowers cited lack of transparency about interest accrual, loan forgiveness options, or the true amount owed, leading to confusion and default.\\n   \\nIn summary, the failures to pay back loans stem from systemic mismanagement, poor communication, and complex economic and procedural barriers that made timely repayment difficult or impossible for borrowers.\""
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to be problems related to loan servicing, including miscommunication, inaccurate or inconsistent information about loan status or payment requirements, and difficulties with repayment plans. Specifically, many complaints mention issues such as:\\n\\n- Struggling to repay loans or problems with forgiveness or discharge.\\n- Errors or discrepancies in loan account information, such as default notices when borrowers have not defaulted.\\n- Confusion about loan servicer or issuer notices.\\n- Problems setting up or maintaining auto-debit payments.\\n- Disputes over inaccurate reporting or illegal collection activities.\\n- Lack of transparency and inadequate communication from loan servicers.\\n\\nTherefore, a common and recurring problem is the mishandling of loan information and servicing issues, which often lead to borrower frustration, incorrect account statuses, and difficulty managing repayment plans.'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, according to the provided complaints, some complaints were handled promptly with responses marked as \"Yes\" for timely response and \"Closed with explanation.\" However, multiple complaints indicate that the complaints were eventually closed with explanations, and in some cases, the complainants reported ongoing issues or violations, suggesting that not all complaints may have been fully resolved in a timely or satisfactory manner. \\n\\nBased on the information, it appears that at least some complaints were not fully handled in a timely manner, or at minimum, there are concerns about the effectiveness and completeness of the resolution process for certain complaints.'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans due to various reasons such as receiving bad or misleading information about their loan status or repayment terms, experiencing technical issues or lack of transparency with their loan servicers, and encountering difficulties in verifying or discharging their debts due to illegal reporting or administrative errors. In some cases, borrowers believe their loans are invalid or unenforceable because of violations of privacy laws, administrative mishandling, or changes in legal status of the loans, which can lead to delays or failures in repayment.'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
        "\n",
        "##### ✅ **Answer:**\n",
        "\n",
        "Interquartile Range (IQR) is likely the best choice for FAQ content for several reasons:\n",
        "\n",
        "- Robustness to outliers: FAQs often contain a few highly similar pairs and some completely distinct ones. IQR handles this distribution better than standard deviation, which gets skewed by extreme values.\n",
        "- Adaptive to content distribution: Unlike percentile methods that impose fixed cut-offs, IQR adapts to the actual similarity distribution in your FAQ dataset.\n",
        "- Interpretable boundaries: IQR provides clearer decision boundaries for what constitutes \"typical\" vs \"unusual\" similarity levels in repetitive content.\n",
        "\n",
        "The change of thresholding method is likely not sufficient for FAQs. In addition, we'd want to combat over-fragmentation, similarity inflation and weak boundary detection problems. A few ideas for algorithmic changes:\n",
        "\n",
        "- Content-aware preprocessing: Implement FAQ-specific parsing to identify question-answer pairs as atomic units. Treat each Q&A pair as the minimum chunk size, preventing fragmentation of logically connected content.\n",
        "- Similarity normalization: Apply domain-specific adjustments to similarity thresholds. Since FAQ language is inherently repetitive, we need to recalibrate what constitutes \"semantically different\" content in this context.\n",
        "- Structural awareness: Leverage FAQ formatting patterns (numbered lists, consistent question structures) to inform chunking decisions rather than relying solely on semantic similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# 🤝 Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### 🏗️ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against each other.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        "    - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        "    - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "##### ✅ **Answer:**\n",
        "\n",
        "1. Generate \"golden dataset\" using knowledge graph, SDG with Ragas\n",
        "    - I installed `ragas` and dependencies\n",
        "2. Pick specific metrics for retrievers\n",
        "    - Select metrics focused on assessing retrieval (ignore generation)\n",
        "3. Set up tracing and evaluation\n",
        "    - LangSmith and helper functions\n",
        "4. Compile the results and write about findings\n",
        "    - Summarize overall findings and lessons learned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5818b70bf2d548e0984d73f9dbc1eb5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/31 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e19f5230a8f47238e6b87217321859e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Node 0f9d1ad9-86cc-4c1c-be9f-e6c971027f73 does not have a summary. Skipping filtering.\n",
            "Node d29fa76f-9c25-44ef-a6dc-5a50ecf12361 does not have a summary. Skipping filtering.\n",
            "Node 6839ff94-c542-4ad7-99b0-013fc5c90a15 does not have a summary. Skipping filtering.\n",
            "Node 40f60a69-0416-4dde-83f8-15d68c7de9ef does not have a summary. Skipping filtering.\n",
            "Node 0332087d-0716-45d0-a3b4-a712cd3f8a97 does not have a summary. Skipping filtering.\n",
            "Node 48af3b29-e804-478a-860d-8774470c0dd5 does not have a summary. Skipping filtering.\n",
            "Node d260713f-a7e7-4e12-87a3-d18d29826679 does not have a summary. Skipping filtering.\n",
            "Node 56aad67a-3ad9-4ab5-85c8-3fc65295b8b3 does not have a summary. Skipping filtering.\n",
            "Node 10483b4b-077c-484c-bc44-f141c0c2e5e7 does not have a summary. Skipping filtering.\n",
            "Node 30f86187-f658-40fe-979e-15a039c5aa2e does not have a summary. Skipping filtering.\n",
            "Node 662691fd-4bfe-4b5a-b86a-1834aa60b9de does not have a summary. Skipping filtering.\n",
            "Node bdbf5c28-8fe8-453d-a9cd-92697d90395a does not have a summary. Skipping filtering.\n",
            "Node 834a8cb7-44aa-4e09-84b1-f11ff52cd5b6 does not have a summary. Skipping filtering.\n",
            "Node 1e79ed20-106d-4abd-8193-57d2b29d7303 does not have a summary. Skipping filtering.\n",
            "Node 14235b24-c453-4960-9acf-982c9864f8bf does not have a summary. Skipping filtering.\n",
            "Node 00564a8d-3b47-4d29-8859-f324379b506f does not have a summary. Skipping filtering.\n",
            "Node 0964e8fb-0601-4081-be92-87b15cdb9b82 does not have a summary. Skipping filtering.\n",
            "Node bf5beef0-8ba7-478c-bcb7-6ffbde726335 does not have a summary. Skipping filtering.\n",
            "Node 4b2d3384-e2de-4ef7-8043-4465dd753f54 does not have a summary. Skipping filtering.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed25a04d9d564495a4f37c817c835ffb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/128 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a78eb58c92b40e69fe97c92c6bf6416",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a36d6b02dbd5483c935d4d4b25d60e42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44cea8fc5c324e3db261dc52c301b469",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac15c7eaccd64ec9ba3b3ce5ee05c9b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/21 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### SOLUTION\n",
        "# 1. Generate golden dataset\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.testset.synthesizers import SingleHopSpecificQuerySynthesizer, MultiHopAbstractQuerySynthesizer, MultiHopSpecificQuerySynthesizer\n",
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "# define LLM to generate data/embeddings\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
        "\n",
        "# query_distribution = [\n",
        "#     (SingleHopSpecificQuerySynthesizer(llm=generator_llm), 0.5),\n",
        "#     (MultiHopAbstractQuerySynthesizer(llm=generator_llm), 0.25),\n",
        "#     (MultiHopSpecificQuerySynthesizer(llm=generator_llm), 0.25),\n",
        "# ]\n",
        "\n",
        "# use subset of complaint data from middle set of records\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "golden_dataset = generator.generate_with_langchain_docs(loan_complaint_data[:50], testset_size=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As a student loan borrower actively disputing ...</td>\n",
              "      <td>[The federal student loan COVID-19 forbearance...</td>\n",
              "      <td>The federal student loan COVID-19 forbearance ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aidvantage why my payment wrong and not based ...</td>\n",
              "      <td>[I submitted my annual Income-Driven Repayment...</td>\n",
              "      <td>Aidvantage assigned me a payment amount that i...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Considering the violation of FERPA in my case,...</td>\n",
              "      <td>[My personal and financial data was compromise...</td>\n",
              "      <td>My personal and financial data was compromised...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What are the allegations against EDFinancial S...</td>\n",
              "      <td>[I am writing to formally dispute inaccurate i...</td>\n",
              "      <td>Recent findings by the Consumer Financial Prot...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does the Aid Avantage impact my ability to...</td>\n",
              "      <td>[I am devastated. I would like to report a sit...</td>\n",
              "      <td>The context indicates that the borrower has Ai...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What role does DOGE play in the context of acc...</td>\n",
              "      <td>[On XXXX XXXX XXXX, XXXX XXXX instructed his t...</td>\n",
              "      <td>On XXXX XXXX XXXX, XXXX XXXX instructed his te...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What issues are borrowers experiencing with Ed...</td>\n",
              "      <td>[I have provided documentation relating to my ...</td>\n",
              "      <td>Borrowers, including the individual in the con...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How do the challenges faced by borrowers, such...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nThe federal student loan COVID-19 ...</td>\n",
              "      <td>The challenges faced by borrowers, including t...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How does the dispute about credit report inacc...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nBreach of Contract - All four bran...</td>\n",
              "      <td>The dispute about inaccurate information on th...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How does the mishandling of my student loan ac...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nTo Whom It May Concern, I am writi...</td>\n",
              "      <td>The context shows that EdFinancials continued ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How do the HR support and documentation issues...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI have provided documentation rela...</td>\n",
              "      <td>The HR support provided multiple forms to veri...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>How does the issue with auto-debit setup relat...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI keep setting up auto-debit with ...</td>\n",
              "      <td>The borrower reports that despite receiving co...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>How does the communication between the bank an...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nXX/XX/XXXX I increased the amount ...</td>\n",
              "      <td>The context describes a borrower experiencing ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>How does the transfer of my account to Nelnet ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nThis account was transferred to Ne...</td>\n",
              "      <td>The transfer of your account to Nelnet is conn...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>How Aidvantage not process my IDR and autopay ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI submitted my annual Income-Drive...</td>\n",
              "      <td>I submitted my IDR recertification to Aidvanta...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>How does the CFPB handle complaints about viol...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nBreach of Contract - All four bran...</td>\n",
              "      <td>The CFPB reviews complaints related to violati...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Wha is the issue with DOGE in the context of b...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nBreach of Contract - All four bran...</td>\n",
              "      <td>The issue with DOGE in the context involves th...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>How does the continued reporting of student lo...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nIllegal Student Loan Reporting &amp; C...</td>\n",
              "      <td>The continued reporting of student loans that ...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>How does the student loan dispute involving Ai...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI am devastated. I would like to r...</td>\n",
              "      <td>The borrower reports that their student loans,...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>How does the dispute over the student loan acc...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI received an alert from XXXX XXXX...</td>\n",
              "      <td>The dispute highlights issues occurring around...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Considering the issues with rejected payments ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI have been making monthly payment...</td>\n",
              "      <td>The borrower is requesting that all future com...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0   As a student loan borrower actively disputing ...   \n",
              "1   Aidvantage why my payment wrong and not based ...   \n",
              "2   Considering the violation of FERPA in my case,...   \n",
              "3   What are the allegations against EDFinancial S...   \n",
              "4   How does the Aid Avantage impact my ability to...   \n",
              "5   What role does DOGE play in the context of acc...   \n",
              "6   What issues are borrowers experiencing with Ed...   \n",
              "7   How do the challenges faced by borrowers, such...   \n",
              "8   How does the dispute about credit report inacc...   \n",
              "9   How does the mishandling of my student loan ac...   \n",
              "10  How do the HR support and documentation issues...   \n",
              "11  How does the issue with auto-debit setup relat...   \n",
              "12  How does the communication between the bank an...   \n",
              "13  How does the transfer of my account to Nelnet ...   \n",
              "14  How Aidvantage not process my IDR and autopay ...   \n",
              "15  How does the CFPB handle complaints about viol...   \n",
              "16  Wha is the issue with DOGE in the context of b...   \n",
              "17  How does the continued reporting of student lo...   \n",
              "18  How does the student loan dispute involving Ai...   \n",
              "19  How does the dispute over the student loan acc...   \n",
              "20  Considering the issues with rejected payments ...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [The federal student loan COVID-19 forbearance...   \n",
              "1   [I submitted my annual Income-Driven Repayment...   \n",
              "2   [My personal and financial data was compromise...   \n",
              "3   [I am writing to formally dispute inaccurate i...   \n",
              "4   [I am devastated. I would like to report a sit...   \n",
              "5   [On XXXX XXXX XXXX, XXXX XXXX instructed his t...   \n",
              "6   [I have provided documentation relating to my ...   \n",
              "7   [<1-hop>\\n\\nThe federal student loan COVID-19 ...   \n",
              "8   [<1-hop>\\n\\nBreach of Contract - All four bran...   \n",
              "9   [<1-hop>\\n\\nTo Whom It May Concern, I am writi...   \n",
              "10  [<1-hop>\\n\\nI have provided documentation rela...   \n",
              "11  [<1-hop>\\n\\nI keep setting up auto-debit with ...   \n",
              "12  [<1-hop>\\n\\nXX/XX/XXXX I increased the amount ...   \n",
              "13  [<1-hop>\\n\\nThis account was transferred to Ne...   \n",
              "14  [<1-hop>\\n\\nI submitted my annual Income-Drive...   \n",
              "15  [<1-hop>\\n\\nBreach of Contract - All four bran...   \n",
              "16  [<1-hop>\\n\\nBreach of Contract - All four bran...   \n",
              "17  [<1-hop>\\n\\nIllegal Student Loan Reporting & C...   \n",
              "18  [<1-hop>\\n\\nI am devastated. I would like to r...   \n",
              "19  [<1-hop>\\n\\nI received an alert from XXXX XXXX...   \n",
              "20  [<1-hop>\\n\\nI have been making monthly payment...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   The federal student loan COVID-19 forbearance ...   \n",
              "1   Aidvantage assigned me a payment amount that i...   \n",
              "2   My personal and financial data was compromised...   \n",
              "3   Recent findings by the Consumer Financial Prot...   \n",
              "4   The context indicates that the borrower has Ai...   \n",
              "5   On XXXX XXXX XXXX, XXXX XXXX instructed his te...   \n",
              "6   Borrowers, including the individual in the con...   \n",
              "7   The challenges faced by borrowers, including t...   \n",
              "8   The dispute about inaccurate information on th...   \n",
              "9   The context shows that EdFinancials continued ...   \n",
              "10  The HR support provided multiple forms to veri...   \n",
              "11  The borrower reports that despite receiving co...   \n",
              "12  The context describes a borrower experiencing ...   \n",
              "13  The transfer of your account to Nelnet is conn...   \n",
              "14  I submitted my IDR recertification to Aidvanta...   \n",
              "15  The CFPB reviews complaints related to violati...   \n",
              "16  The issue with DOGE in the context involves th...   \n",
              "17  The continued reporting of student loans that ...   \n",
              "18  The borrower reports that their student loans,...   \n",
              "19  The dispute highlights issues occurring around...   \n",
              "20  The borrower is requesting that all future com...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   single_hop_specifc_query_synthesizer  \n",
              "5   single_hop_specifc_query_synthesizer  \n",
              "6   single_hop_specifc_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_abstract_query_synthesizer  \n",
              "9   multi_hop_abstract_query_synthesizer  \n",
              "10  multi_hop_abstract_query_synthesizer  \n",
              "11  multi_hop_abstract_query_synthesizer  \n",
              "12  multi_hop_abstract_query_synthesizer  \n",
              "13  multi_hop_abstract_query_synthesizer  \n",
              "14  multi_hop_specific_query_synthesizer  \n",
              "15  multi_hop_specific_query_synthesizer  \n",
              "16  multi_hop_specific_query_synthesizer  \n",
              "17  multi_hop_specific_query_synthesizer  \n",
              "18  multi_hop_specific_query_synthesizer  \n",
              "19  multi_hop_specific_query_synthesizer  \n",
              "20  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# human review of dataset\n",
        "golden_dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### We have the test dataset created, now let's set up LangSmith for tracing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Set up for tracing\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "from uuid import uuid4\n",
        "\n",
        "# Get environment variables\n",
        "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")\n",
        "\n",
        "# Settings for LangSmith\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM - Compare Retrievers - {uuid4().hex[0:8]}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Pick metrics to use\n",
        "# 3. Helper functions (cont.)\n",
        "\n",
        "from langchain.callbacks.tracers import LangChainTracer\n",
        "from ragas import EvaluationDataset, evaluate, RunConfig\n",
        "from ragas.metrics import LLMContextRecall, ContextEntityRecall, LLMContextPrecisionWithReference, NonLLMContextPrecisionWithReference\n",
        "\n",
        "eval_llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "\n",
        "# Helper function to run the tests\n",
        "def test_retriever(name, retriever, results_dict):\n",
        "    \"\"\"test each retriever and return ragas dataset\"\"\"\n",
        "    print(f\"Evaluating {name}...\")\n",
        "\n",
        "    for test_row in golden_dataset:\n",
        "        retrieved_docs = retriever.invoke(test_row.eval_sample.user_input)\n",
        "        test_row.eval_sample.retrieved_contexts = [doc.page_content for doc in retrieved_docs]\n",
        "\n",
        "    eval_dataset = EvaluationDataset.from_pandas(golden_dataset.to_pandas())\n",
        "\n",
        "    tracer = LangChainTracer(project_name=f\"{os.environ[\"LANGCHAIN_PROJECT\"]} - {name}\")\n",
        "\n",
        "    result = evaluate(\n",
        "        dataset=eval_dataset,\n",
        "        # metrics=[LLMContextRecall(),ContextEntityRecall(),LLMContextPrecisionWithReference(),NonLLMContextPrecisionWithReference()],\n",
        "        metrics=[LLMContextRecall(),LLMContextPrecisionWithReference(),NonLLMContextPrecisionWithReference()],\n",
        "        llm=eval_llm,\n",
        "        run_config=RunConfig(timeout=360),\n",
        "        callbacks=[tracer]\n",
        "    )\n",
        "\n",
        "    results_dict[name] = result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Naive...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5479efd0926348c881b6fc1e447db811",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 3. Perform evaluations (cont.)\n",
        "\n",
        "evaluation_result = {}\n",
        "\n",
        "naive_eval = {}\n",
        "test_retriever(\"Naive\", naive_retriever, naive_eval)\n",
        "evaluation_result.update(naive_eval)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating BM25...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a351539181b4aa4bbfa0d3823ba3703",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Ensemble...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ce58a7330344d3ab14c857dd9b808c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bm25_eval = {}\n",
        "test_retriever(\"BM25\", bm25_retriever, bm25_eval)\n",
        "evaluation_result.update(bm25_eval)\n",
        "\n",
        "ensemble_eval = {}\n",
        "test_retriever(\"Ensemble\", ensemble_retriever, ensemble_eval)\n",
        "evaluation_result.update(ensemble_eval)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Multi-Query...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "143e04255b464d86886d17b136e722ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Parent-Document...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8dd3548eacc748f4871fcb0aebf245a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Reranker...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dce668dd4a05443dade864bdf7227f16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "multi_query_eval = {}\n",
        "test_retriever(\"Multi-Query\", multi_query_retriever, multi_query_eval)\n",
        "evaluation_result.update(multi_query_eval)\n",
        "\n",
        "parent_eval = {}\n",
        "test_retriever(\"Parent-Document\", parent_document_retriever, parent_eval)\n",
        "evaluation_result.update(parent_eval)\n",
        "\n",
        "rerank_eval = {}\n",
        "test_retriever(\"Reranker\", compression_retriever, rerank_eval)\n",
        "evaluation_result.update(rerank_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Naive': {'context_recall': 0.8452, 'llm_context_precision_with_reference': 0.7597, 'non_llm_context_precision_with_reference': 0.5369}, 'BM25': {'context_recall': 0.7659, 'llm_context_precision_with_reference': 0.8307, 'non_llm_context_precision_with_reference': 0.6058}, 'Ensemble': {'context_recall': 0.9444, 'llm_context_precision_with_reference': 0.7796, 'non_llm_context_precision_with_reference': 0.6120}, 'Multi-Query': {'context_recall': 0.9167, 'llm_context_precision_with_reference': 0.7837, 'non_llm_context_precision_with_reference': 0.4588}, 'Parent-Document': {'context_recall': 0.6825, 'llm_context_precision_with_reference': 0.8399, 'non_llm_context_precision_with_reference': 0.4153}, 'Reranker': {'context_recall': 0.7341, 'llm_context_precision_with_reference': 0.9444, 'non_llm_context_precision_with_reference': 0.6230}}\n"
          ]
        }
      ],
      "source": [
        "# 4. Get results and analyze\n",
        "\n",
        "print(evaluation_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> NOTE: Completed evaluations serially above and had to drop evaluation for `ContextEntityRecall` due to repeated timeouts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Retriever</th>\n",
              "      <th>Context Recall</th>\n",
              "      <th>LLM Precision</th>\n",
              "      <th>Non-LLM Precision</th>\n",
              "      <th>Prompt Tokens</th>\n",
              "      <th>Output Tokens</th>\n",
              "      <th>P50 Latency</th>\n",
              "      <th>Cost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BM25</td>\n",
              "      <td>0.7659</td>\n",
              "      <td>0.8307</td>\n",
              "      <td>0.6058</td>\n",
              "      <td>191,327</td>\n",
              "      <td>13,238</td>\n",
              "      <td>15s 950ms</td>\n",
              "      <td>$0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ensemble</td>\n",
              "      <td>0.9444</td>\n",
              "      <td>0.7796</td>\n",
              "      <td>0.6120</td>\n",
              "      <td>783,069</td>\n",
              "      <td>43,998</td>\n",
              "      <td>3m 24s 960ms</td>\n",
              "      <td>$0.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Multi-Query</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>0.7837</td>\n",
              "      <td>0.4588</td>\n",
              "      <td>530,502</td>\n",
              "      <td>32,201</td>\n",
              "      <td>2m 33s 580ms</td>\n",
              "      <td>$0.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Naive</td>\n",
              "      <td>0.8452</td>\n",
              "      <td>0.7364</td>\n",
              "      <td>0.5369</td>\n",
              "      <td>371,961</td>\n",
              "      <td>23,633</td>\n",
              "      <td>1m 2s 970ms</td>\n",
              "      <td>$0.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Parent-Document</td>\n",
              "      <td>0.6825</td>\n",
              "      <td>0.8399</td>\n",
              "      <td>0.4153</td>\n",
              "      <td>182,662</td>\n",
              "      <td>12,585</td>\n",
              "      <td>16s 970ms</td>\n",
              "      <td>$0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Reranker</td>\n",
              "      <td>0.7341</td>\n",
              "      <td>0.9444</td>\n",
              "      <td>0.6230</td>\n",
              "      <td>139,389</td>\n",
              "      <td>11,154</td>\n",
              "      <td>22s 910ms</td>\n",
              "      <td>$0.06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Retriever  Context Recall  LLM Precision  Non-LLM Precision  \\\n",
              "0             BM25          0.7659         0.8307             0.6058   \n",
              "1         Ensemble          0.9444         0.7796             0.6120   \n",
              "2      Multi-Query          0.9167         0.7837             0.4588   \n",
              "3            Naive          0.8452         0.7364             0.5369   \n",
              "4  Parent-Document          0.6825         0.8399             0.4153   \n",
              "5         Reranker          0.7341         0.9444             0.6230   \n",
              "\n",
              "  Prompt Tokens Output Tokens   P50 Latency   Cost  \n",
              "0       191,327        13,238     15s 950ms  $0.09  \n",
              "1       783,069        43,998  3m 24s 960ms  $0.34  \n",
              "2       530,502        32,201  2m 33s 580ms  $0.23  \n",
              "3       371,961        23,633   1m 2s 970ms  $0.14  \n",
              "4       182,662        12,585     16s 970ms  $0.07  \n",
              "5       139,389        11,154     22s 910ms  $0.06  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to the CSV file\n",
        "csv_file_path = './retriever_comparison.csv'\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "try:\n",
        "    comparison_df = pd.read_csv(csv_file_path)\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(f\"CSV file not found at path: {csv_file_path}\")\n",
        "except pd.errors.EmptyDataError:\n",
        "    raise ValueError(f\"CSV file at {csv_file_path} is empty.\")\n",
        "except pd.errors.ParserError as e:\n",
        "    raise ValueError(f\"Error parsing CSV file at {csv_file_path}: {e}\")\n",
        "\n",
        "# Display the DataFrame as a table\n",
        "display(comparison_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Findings\n",
        "\n",
        "Raw results are in the data table above. The results were assembled from LangSmith traces. Below is a screen capture of the output of an HTML page where you can explore the rankings depending on the most important factors: accuracy, speed, cost, balance. This is the \"balanced\" result. Open `retriever_scoring_calculator.html` in a web browser to explore.\n",
        "\n",
        "This exercise forced me to ask: \"What is the job of a retriever and what makes it good?\"\n",
        "\n",
        "1. Recall: is all the information needed to answer the question comprehensively present in the retrieved context?\n",
        "2. Precision: how many of the retrieved chunks are actually relevant to answering the question?\n",
        "3. Cost: how many tokens (input & output) are used?\n",
        "4. Speed: how long do we wait for the response?\n",
        "\n",
        "By all of those measures, the `compression_retriever` that uses Cohere's Rerank is the winner, with BM25 as a decent second option.\n",
        "\n",
        "<img src=\"./balanced_result.png\" alt=\"screen capture of balanced result table\" width=\"738\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Built with help from Claude\n",
        "\n",
        "🎯 Customizable Weights: Adjust the importance of each metric based on your priorities:\n",
        "\n",
        "- Context Recall & Precision: Quality metrics (higher = better)\n",
        "- Cost Efficiency: Inverted cost metric (lower cost = higher score)\n",
        "- Speed: Inverted latency metric (faster = higher score)\n",
        "- Token Efficiency: Combined prompt/output token usage (fewer = better)\n",
        "\n",
        "🏆 Smart Scoring:\n",
        "\n",
        "- All metrics are normalized to 0-100 scale for fair comparison\n",
        "- Cost, latency, and token usage are inverted (lower raw values get higher scores)\n",
        "- Final score is the weighted average of all normalized metrics\n",
        "\n",
        "📊 Preset Options:\n",
        "\n",
        "- Accuracy First: Prioritizes recall and precision\n",
        "- Speed First: Emphasizes low latency\n",
        "- Cost First: Focuses on cost efficiency\n",
        "- Balanced: Equal consideration of all factors\n",
        "\n",
        "The formula automatically adapts based on your weight preferences, making it easy to find the best algorithm for your specific use case. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
