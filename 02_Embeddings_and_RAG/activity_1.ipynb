{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lElF3o5PR6ys"
   },
   "source": [
    "# üèóÔ∏è Activity #1: Enhanced RAG App\n",
    "\n",
    "Enhance your RAG application in some way! \n",
    "\n",
    "Suggestions are: \n",
    "\n",
    "- Allow it to work with PDF files\n",
    "- Implement a new distance metric\n",
    "- Add metadata support to the vector database\n",
    "\n",
    "While these are suggestions, you should feel free to make whatever augmentations you desire! \n",
    "\n",
    "> NOTE: These additions might require you to work within the `aimakerspace` library - that's expected!\n",
    "\n",
    "> NOTE: If you're not sure where to start - ask Cursor (CMD/CTRL+L) to guide you through the changes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Add PDF Files\n",
    "\n",
    "**Plan:**\n",
    "\n",
    "1. Copy original RAG application because this is intended to **enhance** original version.\n",
    "    - NOTE; I am only including the code steps in this notebook for brevity, see `Pythonic_RAG_Assignment.ipynb` for the detailed walk through.\n",
    "2. Create new class that handles loading, embedding, and insert into VectorDatabase for multiple PDF files.\n",
    "    - NOTE: This class is added to `aimakerspace` library. It is best practice in data engineering to keep like file formats together, so I all PDFs are together in a directory.\n",
    "3. Do the standard workflow for the PDFs: collect, chunk, add to vector database.\n",
    "    - NOTE: I leave the prompts as-is to assess initial performace with the PDF data addition only.\n",
    "4. Test retrieval.\n",
    "5. Assess result and conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjmC0KFtR6yt"
   },
   "source": [
    "--- \n",
    "\n",
    "For simplicity, we will merge some cells and provide quick comments on the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Z1dyrG4hR6yt"
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from aimakerspace.text_utils import TextFileLoader, CharacterTextSplitter\n",
    "from aimakerspace.vectordatabase import VectorDatabase\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ia2sUEuGR6yu",
    "outputId": "84937ecc-c35f-4c4a-a4ab-9da72625954c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document.\n",
      "\n",
      "Here's an excerpt: Ôªø\n",
      "The Pmarca Blog Archives\n",
      "(select posts from 2007-2009)\n",
      "Marc Andreessen\n",
      "copyright: Andreessen Horow\n"
     ]
    }
   ],
   "source": [
    "# load text document\n",
    "text_loader = TextFileLoader(\"data/PMarcaBlogs.txt\")\n",
    "documents = text_loader.load_documents()\n",
    "\n",
    "# sanity check\n",
    "print(f\"Loaded {len(documents)} document.\")\n",
    "print(f\"\\nHere's an excerpt: {documents[0][:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMC4tsEmR6yv",
    "outputId": "08689c0b-57cd-4040-942a-8193e997f5cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 373 chunks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\ufeff\\nThe Pmarca Blog Archives\\n(select posts from 2007-2009)\\nMarc Andreessen\\ncopyright: Andreessen Horowitz\\ncover design: Jessica Hagy\\nproduced using: Pressbooks\\nContents\\nTHE PMARCA GUIDE TO STARTUPS\\nPart 1: Why not to do a startup 2\\nPart 2: When the VCs say \"no\" 10\\nPart 3: \"But I don\\'t know any VCs!\" 18\\nPart 4: The only thing that matters 25\\nPart 5: The Moby Dick theory of big companies 33\\nPart 6: How much funding is too little? Too much? 41\\nPart 7: Why a startup\\'s initial business plan doesn\\'t\\nmatter that much\\n49\\nTHE PMARCA GUIDE TO HIRING\\nPart 8: Hiring, managing, promoting, and Dring\\nexecutives\\n54\\nPart 9: How to hire a professional CEO 68\\nHow to hire the best people you\\'ve ever worked\\nwith\\n69\\nTHE PMARCA GUIDE TO BIG COMPANIES\\nPart 1: Turnaround! 82\\nPart 2: Retaining great people 86\\nTHE PMARCA GUIDE TO CAREER, PRODUCTIVITY,\\nAND SOME OTHER THINGS\\nIntroduction 97\\nPart 1: Opportunity 99\\nPart 2: Skills and education 107\\nPart 3: Where to go and why 120\\nThe Pmarca Guide to Personal Productivi']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chunk the loaded text\n",
    "text_splitter = CharacterTextSplitter()\n",
    "split_documents = text_splitter.split_texts(documents)\n",
    "\n",
    "# sanity check\n",
    "print(f\"Split into {len(split_documents)} chunks.\")\n",
    "split_documents[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input our OpenAI API key to do the API calls\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "openai.api_key = getpass(\"OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "O4KoLbVDR6yv"
   },
   "outputs": [],
   "source": [
    "# instantiate the vector database and insert embeddings\n",
    "vector_db = VectorDatabase()\n",
    "vector_db = asyncio.run(vector_db.abuild_from_list(split_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76d96uavR6yw",
    "outputId": "bbfccc31-20a2-41c7-c14d-46554a43ed2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ordingly.\\nSeventh, when hiring the executive to run your former specialty, be\\ncareful you don‚Äôt hire someone weak on purpose.\\nThis sounds silly, but you wouldn‚Äôt believe how oaen it happens.\\nThe CEO who used to be a product manager who has a weak\\nproduct management executive. The CEO who used to be in\\nsales who has a weak sales executive. The CEO who used to be\\nin marketing who has a weak marketing executive.\\nI call this the ‚ÄúMichael Eisner Memorial Weak Executive Problem‚Äù ‚Äî aaer the CEO of Disney who had previously been a brilliant TV network executive. When he bought ABC at Disney, it\\npromptly fell to fourth place. His response? ‚ÄúIf I had an extra\\ntwo days a week, I could turn around ABC myself.‚Äù Well, guess\\nwhat, he didn‚Äôt have an extra two days a week.\\nA CEO ‚Äî or a startup founder ‚Äî oaen has a hard time letting\\ngo of the function that brought him to the party. The result: you\\nhire someone weak into the executive role for that function so\\nthat you can continue to be ‚Äúthe man‚Äù ‚Äî cons',\n",
       "  np.float64(0.6538563767462544)),\n",
       " ('m. They have areas where they are truly deXcient in judgment or skill set. That‚Äôs just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus on strength rather than lack of weakness. Everybody has severe weaknesses even if you can‚Äôt see\\nthem yet. When managing, it‚Äôs oaen useful to micromanage and\\nto provide remedial training around these weaknesses. Doing so\\nmay make the diWerence between an executive succeeding or\\nfailing.\\nFor example, you might have a brilliant engineering executive\\nwho generates excellent team loyalty, has terriXc product judgment and makes the trains run on time. This same executive\\nmay be very poor at relating to the other functions in the company. She may generate far more than her share of cross-functional conYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n(a) Macro-manage and give her an annual or quarterly object',\n",
       "  np.float64(0.5036012174947992)),\n",
       " ('ed?\\nIn reality ‚Äî as opposed to Marc‚Äôs warped view of reality ‚Äî it will\\nbe extremely helpful for Marc [if he were actually the CEO,\\nwhich he is not] to meet with the new head of engineering daily\\nwhen she comes on board and review all of her thinking and\\ndecisions. This level of micromanagement will accelerate her\\ntraining and improve her long-term eWectiveness. It will make\\nher seem smarter to the rest of the organization which will build\\ncredibility and conXdence while she comes up to speed. Micromanaging new executives is generally a good idea for a limited\\nperiod of time.\\nHowever, that is not the only time that it makes sense to micro66 The Pmarca Blog Archives\\nmanage executives. It turns out that just about every executive\\nin the world has a few things that are seriously wrong with\\nthem. They have areas where they are truly deXcient in judgment or skill set. That‚Äôs just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus o',\n",
       "  np.float64(0.4814102594977529))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "vector_db.search_by_text(\"What is the Michael Eisner Memorial Weak Executive Problem?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up for communication with OpenAI chat\n",
    "from aimakerspace.openai_utils.prompts import (\n",
    "    UserRolePrompt,\n",
    "    SystemRolePrompt,\n",
    ")\n",
    "\n",
    "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
    "\n",
    "# instantiate OpenAI client\n",
    "chat_openai = ChatOpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "D1hamzGaR6yy"
   },
   "outputs": [],
   "source": [
    "RAG_SYSTEM_TEMPLATE = \"\"\"You are a knowledgeable assistant that answers questions based strictly on provided context.\n",
    "\n",
    "Instructions:\n",
    "- Only answer questions using information from the provided context\n",
    "- If the context doesn't contain relevant information, respond with \"I don't know\"\n",
    "- Be accurate and cite specific parts of the context when possible\n",
    "- Keep responses {response_style} and {response_length}\n",
    "- Only use the provided context. Do not use external knowledge.\n",
    "- Only provide answers when you are confident the context supports your response.\"\"\"\n",
    "\n",
    "RAG_USER_TEMPLATE = \"\"\"Context Information:\n",
    "{context}\n",
    "\n",
    "Number of relevant sources found: {context_count}\n",
    "{similarity_scores}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Please provide your answer based solely on the context above.\"\"\"\n",
    "\n",
    "rag_system_prompt = SystemRolePrompt(\n",
    "    RAG_SYSTEM_TEMPLATE,\n",
    "    strict=True,\n",
    "    defaults={\n",
    "        \"response_style\": \"concise\",\n",
    "        \"response_length\": \"brief\"\n",
    "    }\n",
    ")\n",
    "\n",
    "rag_user_prompt = UserRolePrompt(\n",
    "    RAG_USER_TEMPLATE,\n",
    "    strict=True,\n",
    "    defaults={\n",
    "        \"context_count\": \"\",\n",
    "        \"similarity_scores\": \"\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG pipeline\n",
    "class RetrievalAugmentedQAPipeline:\n",
    "    def __init__(self, llm: ChatOpenAI, vector_db_retriever: VectorDatabase,\n",
    "                 response_style: str = \"detailed\", include_scores: bool = False) -> None:\n",
    "        self.llm = llm\n",
    "        self.vector_db_retriever = vector_db_retriever\n",
    "        self.response_style = response_style\n",
    "        self.include_scores = include_scores\n",
    "\n",
    "    def run_pipeline(self, user_query: str, k: int = 4, **system_kwargs) -> dict:\n",
    "        # Retrieve relevant contexts\n",
    "        context_list = self.vector_db_retriever.search_by_text(user_query, k=k)\n",
    "\n",
    "        context_prompt = \"\"\n",
    "        similarity_scores = []\n",
    "\n",
    "        for i, (context, score) in enumerate(context_list, 1):\n",
    "            context_prompt += f\"[Source {i}]: {context}\\n\\n\"\n",
    "            similarity_scores.append(f\"Source {i}: {score:.3f}\")\n",
    "\n",
    "        # Create system message with parameters\n",
    "        system_params = {\n",
    "            \"response_style\": self.response_style,\n",
    "            \"response_length\": system_kwargs.get(\"response_length\", \"detailed\")\n",
    "        }\n",
    "\n",
    "        formatted_system_prompt = rag_system_prompt.create_message(**system_params)\n",
    "\n",
    "        user_params = {\n",
    "            \"user_query\": user_query,\n",
    "            \"context\": context_prompt.strip(),\n",
    "            \"context_count\": len(context_list),\n",
    "            \"similarity_scores\": f\"Relevance scores: {', '.join(similarity_scores)}\" if self.include_scores else \"\"\n",
    "        }\n",
    "\n",
    "        formatted_user_prompt = rag_user_prompt.create_message(**user_params)\n",
    "\n",
    "        return {\n",
    "            \"response\": self.llm.run([formatted_system_prompt, formatted_user_prompt]),\n",
    "            \"context\": context_list,\n",
    "            \"context_count\": len(context_list),\n",
    "            \"similarity_scores\": similarity_scores if self.include_scores else None,\n",
    "            \"prompts_used\": {\n",
    "                \"system\": formatted_system_prompt,\n",
    "                \"user\": formatted_user_prompt\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define it\n",
    "rag_pipeline = RetrievalAugmentedQAPipeline(\n",
    "    vector_db_retriever=vector_db,\n",
    "    llm=chat_openai,\n",
    "    response_style=\"detailed\",\n",
    "    include_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The 'Michael Eisner Memorial Weak Executive Problem' refers to the phenomenon where a CEO or startup founder, due to their background in a specific functional area (like product management, sales, or marketing), hires a weak executive for that area on purpose. This often occurs because the CEO has difficulty letting go of the function that initially contributed to their success. As a result, the CEO may opt for a less competent executive in order to maintain their role as the dominant figure in that area. The term is named after Michael Eisner, the former CEO of Disney, who faced challenges when he bought ABC and saw it fall to fourth place, despite believing he could turn it around if he had more time (Source 1).\n",
      "\n",
      "Context Count: 3\n",
      "Similarity Scores: ['Source 1: 0.658', 'Source 2: 0.509', 'Source 3: 0.479']\n"
     ]
    }
   ],
   "source": [
    "# run it with a query\n",
    "result = rag_pipeline.run_pipeline(\n",
    "    \"What is the 'Michael Eisner Memorial Weak Executive Problem'?\",\n",
    "    k=3,\n",
    "    response_length=\"comprehensive\",\n",
    "    include_warnings=True,\n",
    "    confidence_required=True\n",
    ")\n",
    "\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"\\nContext Count: {result['context_count']}\")\n",
    "print(f\"Similarity Scores: {result['similarity_scores']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Activity #1: Enhance RAG Application\n",
    "\n",
    "**Add PDFs to App**\n",
    "\n",
    "I found a [GitHub repo](https://github.com/tpn/pdfs) with a bunch of old PDFs. I selected 10 with the smallest file size to keep things simple, and saved them into the `/data/pdfs` directory. In data engineering it's best practice to keep like data types together if we can.\n",
    "\n",
    "Next, I used `ask` mode with Cursor to create the `PDFFileLoader` class in the `text_utils.py` file. I added a new library `PyPDF2` which is referenced in `pyproject.toml` as a dependency to handle the processing of PDFs. I also incremented the project to version `0.2.0` since the ability to ingest PDFs is a minor enhancement.\n",
    "\n",
    "The assignment is an exercise to augment the existing app and so we add the embeddings to our vector database so they are available for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 documents.\n",
      "\n",
      "Here's an excerpt:\n",
      "COMP 423 lecture 6 Jan. 16, 2008\n",
      "Codes for the positive integers\n",
      "There are many situations in which \n"
     ]
    }
   ],
   "source": [
    "# import my new class and instantiate it then load the pdfs\n",
    "from aimakerspace.text_utils import PDFFileLoader\n",
    "\n",
    "pdf_loader = PDFFileLoader(\"data/pdfs\")  # directory containing PDF files\n",
    "pdf_documents = pdf_loader.load_documents()\n",
    "\n",
    "# sanity check\n",
    "print(f\"Loaded {len(pdf_documents)} documents.\") # Number of PDFs loaded\n",
    "print(f\"\\nHere's an excerpt:\\n{pdf_documents[2][:100]}\") # Show first hundred chars of pdf 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 289 chunks.\n"
     ]
    }
   ],
   "source": [
    "# chunk the pdfs\n",
    "pdf_text_splitter = CharacterTextSplitter()\n",
    "split_pdfs = text_splitter.split_texts(pdf_documents)\n",
    "len(split_pdfs)\n",
    "\n",
    "# how many chunks?\n",
    "print(f\"Split into {len(split_pdfs)} chunks.\")\n",
    "\n",
    "# add chunks to the vector_db\n",
    "vector_db = asyncio.run(vector_db.abuild_from_list(split_pdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('st time the client has cached it\\n305 Use Proxy The resource should be accessed through a speci\\x0ced proxy\\n307 Temporary Redirect The request should be repeated with the same request method at the given address. Added in HTTP/1.1 to clarify the ambiguity in the behavior of status\\n302. See 302 and 303\\n4XX Client Error\\nStatus Code Description\\n400 Bad Request The request can not be ful\\x0clled because the request contained bad syntax\\n401 Unauthorized The client needs to authenticate in order to access this resource\\n402 Payment Required This code is intended to be used for a micropayment system, but the speci\\x0ccs for this system are unspeci\\x0ced and this code is rarely used\\n403 Forbidden The client is not allowed to access this resource. Generally, the client is authenticated and does not have su\\x0ecient permission\\n404 Not Found The resource was not found, though its existence in the future is possible\\n405 Method Not Allowed The method used in the request is not supported by the resource\\n406 Not Acce',\n",
       "  np.float64(0.5557467194933978)),\n",
       " (\"KNOW YOUR HTTP STATUS CODES!HTTP status codes are returned in the response. They each consist of a three digit numerical code and a text description (the text description is simply advisory, and may be\\ntranslated to other languages). The codes are categorized such that general classes of errors have the same most-signi\\x0ccant digit, so if a client does not recognize a status code it\\nmay still infer the code's category.\\n1XX Informational\\nStatus Code Description\\n100 Continue The server has received the request headers, and the client should begin to send the request body\\n101 Switching Protocols The server has received a request to switch protocols and is doing so\\n102 Processing (WebDAV) Indicates that the server has received the request, used by WebDAV to avoid timeouts for long-running requests\\n2XX Success\\nStatus Code Description\\n200 OK Standard response for successful HTTP requests\\n201 Created The request was successful, and a new resource has been created\\n202 Accepted The request was ac\",\n",
       "  np.float64(0.5507736678833204)),\n",
       " (\"ts\\n2XX Success\\nStatus Code Description\\n200 OK Standard response for successful HTTP requests\\n201 Created The request was successful, and a new resource has been created\\n202 Accepted The request was accepted for processing, but the job hasn't actually been completed. It is possible that the request will be rejected once processing takes\\nplace\\n203 Non-Authoritative Information The request was successfully processed, but the returned information may be from an untrusted third party\\n204 No Content The request was processed and no content is being returned\\n206 Partial Content Only part of the request body is being delivered (for example, when resuming an interrupted download)\\n207 Multi-Status (WebDAV) The message body is an XML document and may contain multiple status codes per sub-requests\\n208 Already Reported (WebDAV) The response has already been enumerated in a previous reply and will not be reported again\\n3XX Redirection\\nStatus Code Description\\n300 Multiple Choices Indicates that there\",\n",
       "  np.float64(0.5221728370836612))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check with direct db query\n",
    "vector_db.search_by_text(\"What do http status codes starting with 4 mean?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ordingly.\\nSeventh, when hiring the executive to run your former specialty, be\\ncareful you don‚Äôt hire someone weak on purpose.\\nThis sounds silly, but you wouldn‚Äôt believe how oaen it happens.\\nThe CEO who used to be a product manager who has a weak\\nproduct management executive. The CEO who used to be in\\nsales who has a weak sales executive. The CEO who used to be\\nin marketing who has a weak marketing executive.\\nI call this the ‚ÄúMichael Eisner Memorial Weak Executive Problem‚Äù ‚Äî aaer the CEO of Disney who had previously been a brilliant TV network executive. When he bought ABC at Disney, it\\npromptly fell to fourth place. His response? ‚ÄúIf I had an extra\\ntwo days a week, I could turn around ABC myself.‚Äù Well, guess\\nwhat, he didn‚Äôt have an extra two days a week.\\nA CEO ‚Äî or a startup founder ‚Äî oaen has a hard time letting\\ngo of the function that brought him to the party. The result: you\\nhire someone weak into the executive role for that function so\\nthat you can continue to be ‚Äúthe man‚Äù ‚Äî cons',\n",
       "  np.float64(0.6538563767462544)),\n",
       " ('m. They have areas where they are truly deXcient in judgment or skill set. That‚Äôs just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus on strength rather than lack of weakness. Everybody has severe weaknesses even if you can‚Äôt see\\nthem yet. When managing, it‚Äôs oaen useful to micromanage and\\nto provide remedial training around these weaknesses. Doing so\\nmay make the diWerence between an executive succeeding or\\nfailing.\\nFor example, you might have a brilliant engineering executive\\nwho generates excellent team loyalty, has terriXc product judgment and makes the trains run on time. This same executive\\nmay be very poor at relating to the other functions in the company. She may generate far more than her share of cross-functional conYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n(a) Macro-manage and give her an annual or quarterly object',\n",
       "  np.float64(0.5036012174947992)),\n",
       " ('ed?\\nIn reality ‚Äî as opposed to Marc‚Äôs warped view of reality ‚Äî it will\\nbe extremely helpful for Marc [if he were actually the CEO,\\nwhich he is not] to meet with the new head of engineering daily\\nwhen she comes on board and review all of her thinking and\\ndecisions. This level of micromanagement will accelerate her\\ntraining and improve her long-term eWectiveness. It will make\\nher seem smarter to the rest of the organization which will build\\ncredibility and conXdence while she comes up to speed. Micromanaging new executives is generally a good idea for a limited\\nperiod of time.\\nHowever, that is not the only time that it makes sense to micro66 The Pmarca Blog Archives\\nmanage executives. It turns out that just about every executive\\nin the world has a few things that are seriously wrong with\\nthem. They have areas where they are truly deXcient in judgment or skill set. That‚Äôs just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus o',\n",
       "  np.float64(0.4814102594977529))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "# ensure we are augmenting and did not remove the existing text!\n",
    "vector_db.search_by_text(\"What is the Michael Eisner Memorial Weak Executive Problem?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: HTTP status codes starting with 4 indicate a \"Client Error.\" These codes represent situations where the client's request contains an error or the client does not have the required permissions for the resource. The relevant codes from the provided context are:\n",
      "\n",
      "- **400 Bad Request**: The request cannot be fulfilled because it contained bad syntax.\n",
      "- **401 Unauthorized**: The client needs to authenticate to access this resource.\n",
      "- **402 Payment Required**: This code is intended for a micropayment system, but the specifics are unspecified, and it is rarely used.\n",
      "- **403 Forbidden**: The client is not allowed to access this resource, typically because they are authenticated but do not have sufficient permission.\n",
      "- **404 Not Found**: The resource was not found, though its existence in the future is possible.\n",
      "- **405 Method Not Allowed**: The method used in the request is not supported by the resource.\n",
      "\n",
      "This classification is specifically outlined in Source 1 of the provided context.\n",
      "\n",
      "Context Count: 3\n",
      "Similarity Scores: ['Source 1: 0.556', 'Source 2: 0.551', 'Source 3: 0.522']\n"
     ]
    }
   ],
   "source": [
    "# run enhanced RAG pipeline with a query\n",
    "result = rag_pipeline.run_pipeline(\n",
    "    \"What do http status codes starting with 4 mean?\",\n",
    "    k=3,\n",
    "    response_length=\"comprehensive\",\n",
    "    include_warnings=True,\n",
    "    confidence_required=True\n",
    ")\n",
    "\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"\\nContext Count: {result['context_count']}\")\n",
    "print(f\"Similarity Scores: {result['similarity_scores']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Up and Learnings\n",
    "\n",
    "With just a few functions, we have a powerful application that is pulling in useful results and respecting the context provided. The structure and annotations on the original notebook made it easy to understand and extend the functionality.\n",
    "\n",
    "### 3 Lessons Learned\n",
    "\n",
    "1. Make informed choices: Choices made at the level of data ingestion and embedding have real effects on the output and should be made with informed knowledge of how your app is to be used. For example: what types of queries do you expect and what are the characteristics of optimal response output?\n",
    "\n",
    "2. Chunk size: Chunking directly affects the quality, accuracy and relevance of the app responses. A good rule of thumb is that chunk size should match the expected query length. More complex queries benefit from larger chunk sizes. We want to strike a balance between gathering semantic meaning of sentences vs. paragraphs for example. In the case of our application, chunk size is set at 1000 tokens which should be appropriate for a page or several long paragraphs of content. \n",
    "    - More thoughts here.[^1]\n",
    "\n",
    "3. Similarity scores: We can use other types of similarity scores besides cosine similarity determine which chunks to retrieve from our vector database for the model to consider. For the purpose of this example app, cosine similarity seems like a good choice.\n",
    "    - Claude gave me lots to read up on in this area! In general the advice seems to be starting with cosine similarity and experimenting with other types if you have an appropriate use case.[^2]\n",
    "\n",
    "### 3 Lessons Not Yet Learned\n",
    "\n",
    "1. Adding and using metadata: I can see how useful it would be to capture data such as author name, document name, date published, category, etc. Once that info is added, I'd want to update the prompt to include this information in a structured way in the response. Can I \"weight\" metadata more heavily to boost scores like one does in search. I bet it's possible!\n",
    "\n",
    "2. Evaluating and improving: The similarity scores are all about the same in this example. How do I know if that is good? What is the process and some metrics to benchmark and continue to improve the results?\n",
    "\n",
    "3. Repeated data: What if I mess up and re-insert data that is already there? Need to read docs to see if this is handled in vector db implementation or if code needs to ensure non-repetitive data.\n",
    "\n",
    "4. Code modularity: I piggybacked off of Chris' awesome library. I need to gain more experience in the right way to think about structuring RAG projects for reuse with Python language. If I were to containerize this, how would I split out the functionality? For example, one change I think I'd make is to move the prompts to their own directory and make several more that could be swapped in/out for experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^1]: More about chunking:\n",
    "\n",
    "- From @Hee-Meng in discord: [Chunking Strategies](https://www.youtube.com/watch?v=ZTOtxiWb2bE) was so helpful! There are at least 5 different strategies for chunking (fixed size, context aware, recursive, specialized, semantic) and the advice given is to first clean your data, select the technique most appropriate for your use case, experiment with chunks of various sizes, and then assess the quality of responses before deciding on the optimal size.\n",
    "\n",
    "[^2]: More similarity metrics:\n",
    "- **Cosine Similarity**\n",
    "    Measures the angle between vectors, ignoring magnitude. Values range from -1 to 1.\n",
    "    _Best for:_ Text embeddings, semantic search, and cases where document length varies significantly. It's the most common choice for RAG because it focuses on meaning rather than word count. Works well when you want documents with similar topics regardless of their length.\n",
    "\n",
    "- **Euclidean Distance (L2)**\n",
    "    Measures straight-line distance between points in vector space. Smaller distances indicate higher similarity.\n",
    "    _Best for:_ When both direction and magnitude matter, such as image embeddings or structured data. Less common in text RAG but useful when the embedding model produces vectors where magnitude carries semantic meaning.\n",
    "\n",
    "- **Dot Product**\n",
    "    Multiplies corresponding vector components and sums them. Higher values indicate greater similarity.\n",
    "    _Best for:_ When vectors are normalized or when you want to consider both similarity and magnitude. Often used in neural networks and when working with embeddings that have been specifically designed for dot product similarity.\n",
    "\n",
    "- **Manhattan Distance (L1)**\n",
    "    Sums absolute differences between corresponding vector components.\n",
    "    _Best for:_ High-dimensional sparse vectors or when you want to reduce the impact of outliers. Less sensitive to extreme values than Euclidean distance. Sometimes used with count-based embeddings.\n",
    "\n",
    "- **Jaccard Similarity**\n",
    "    Measures overlap between sets, calculated as intersection over union.\n",
    "    _Best for:_ Sparse binary vectors, keyword matching, or when working with bag-of-words representations. Useful for document similarity based on shared terms rather than semantic meaning.\n",
    "\n",
    "- **Hamming Distance**\n",
    "    Counts differing positions between binary vectors.\n",
    "    _Best for:_ Binary embeddings, locality-sensitive hashing, or when memory/speed constraints require binary representations. Often used in similarity search optimizations."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1ce393d9afcf427d9d352259c5d32678": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e6efd99f7d346e485b002fb0fa85cc7",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3dfb67c39958461da6071e4c19c3fa41",
      "value": 1
     }
    },
    "3a4ba348cb004f8ab7b2b1395539c81b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2ea5009dd16442cb5d8a0ac468e50a8",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5f00135fe1044051a50ee5e841cbb8e3",
      "value": "0.018 MB of 0.018 MB uploaded\r"
     }
    },
    "3dfb67c39958461da6071e4c19c3fa41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4e6efd99f7d346e485b002fb0fa85cc7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56a8e24025594e5e9ff3b8581c344691": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f00135fe1044051a50ee5e841cbb8e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb904e05ece143c79ecc4f20de482f45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3a4ba348cb004f8ab7b2b1395539c81b",
       "IPY_MODEL_1ce393d9afcf427d9d352259c5d32678"
      ],
      "layout": "IPY_MODEL_56a8e24025594e5e9ff3b8581c344691"
     }
    },
    "d2ea5009dd16442cb5d8a0ac468e50a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
